% Emacs, this is -*-latex-*-

% Data Broadcasting Set of rules

DBS does not imposes any control over the grade of solidarity of the
peers. This means that selfish peers (or simply peers with reduced
connectivity) can stay in the team thanks to the generosity of the
rest of peers, even if they never achive to deliver a chunk to any
peer of the team. This set or rules preclude this possible behavior,
by impossing a minimum degree of solidarity between neighbor peers.

To know the level of solidarity between neighbor peers, each peer uses
a table of chunk debts, $\mathtt{debt}[]$. Every time a peer $P_i$
sends a chunk to $P_j$, $P_i$ increments $\mathtt{debt}[P_j]$, and on
the contrary, decrements $\mathtt{debt}[P_j]$ when $P_i$ receives a
chunk from $P_j$.

Peers forward chunks to their neighbors in the order in which the
entries of $\mathtt{pending}[]$ are accessed (a round-robing
scheduling in DBS). Considering this, FCS modifies this behavior:
\begin{enumerate}
\item To go through $\mathtt{pending}[]$ in the order provided by
  $\mathtt{debt}[]$, selecting first those entries with a smaller
  debts.
\item To reset the run of $\mathtt{pending}[]$ in each round (when a
  chunk is received from the splitter).
\item If $P_i$ realises that $\mathtt{debt}[P_j]>L^*$, $P_i$ removes
  $P_j$ from $\mathtt{forward}[\forall P_k\in\{\text{team}\cup P_i\}]$
  and $\mathtt{pending}[]$. Notice that this action decreases the
  neighborhood degree of $P_i$ and, soon or later of $P_j$ that will
  consider $P_i$ as unsupportive.
\item In DBS, request messages are sent selecting the destination peer
  at random. In FCS, request messages are sent to those peers with a
  higher debt. Thus, if the insolidarity is produced by a overlay
  topology imbalance (an extreme example is in Fig.~\ref{fig:star}),
  badly connected peers peers could have the chance of mitigating this
  problem by forwarding more chunks to their neighbors.
\end{enumerate}

Using FCS, supportive peers will be served first, incrementing the
QoE of the corresponding peers. On the other hand, those peers with a
higher chunk debt will tend to be unserved if no enough bandwidth is
available. 


\begin{comment}
Notice that in this example, for the sake of simplicity, a simple
round-robing pending scheduler has been used.  Actually, DBS selects
the $\mathtt{pending}[]$ entries using the supportivity information
gathered from the neighbors. This information is stored in a
$\mathtt{supportivity}[]$ table, which is indexed by the neighbors
end-points. When the list of peers is received from the splitter, all
the peers have the same supportivity. These supportivity values are
incremented each time a chunk is received from the corresponding
neighbor and decremented each time a chunk is received form the
splitter (i.e., in each round). Thus, supportive neighbors will tend
to have a higher supportivity than unsupportive neighbors.
\end{comment}

\begin{comment}
To achieve this behavior, FCS defines that, if ${\cal P}^j_k$ realises
that $\mathtt{debt}[{\cal P}^j_l]>\mathtt{debt}_{\text{max}}$, then
${\cal P}^j_k$ removes ${\cal P}^j_l$ from ${\cal T}^j_k$. Obviously,
${\cal P}^j_l$ should churn, unless it not interested in playing the
media.

, where those peers with
a low chunk debt are selected first.  Debs are clipped to $\pm
D$. In ideal circunstances, debs should be $0$. Obviously, a high
supportivity means a low debt, and viceversa.

In DBS, the splitter sends to each peer of the team one chunk per
round. On the other hand, the peers can have a variable number of
neighbors. In this context, those peers with a higher degree of
connectivity will forward more chunks than the peers with a lower
degree. So, by definition, peers with a lower connectivity will
forward a lower number of chunks for those peers that it is the origin
peer. In the extreme case, a peer $P_x$ behind a NAT could be
connected only with one external peer $P_y$ which should forward to it
all the chunks except those that receive directly from the
splitter. Obviously, in this case, $\text{debt}[P_x]$ in $P_y$ will
reach $D$ fastly.

Peers will remove as neighbors those peers whose debt reaches $D$
during $D^*$ consecutive rounds.
\end{comment}

\begin{notex}
  The prioritized round-robin neighbor selection has not yet been
  implemented as it has been explained here. The $\text{debt}[]$
  structure exists, but is used for a different purporse.
\end{notex}


\begin{comment}
In each round, peers check if a chunk have been received from the rest
of peers of the team (${\cal P}_k\in {\cal T}_j)$). If not, peers send
a $[\mathtt{propagate}~{\cal P}_i]$ to one or more (possibly
to the rest of) peers of the team, where ${\cal P}_i$ is the origin peer
of the missing chunk. At this point, the process continues as
described in Section~\ref{dbs:chunk_flooding}.
\end{comment}

\begin{comment}
For each ${\cal P}_k\in N({\cal P}_i)$, ${\cal P}_i$ checks if a chunk
has been received from ${\cal P}_k$. If ${\cal P}_i$ detects that
${\cal P}_k$ has not sent a chunk to it during $L$ consecutive rounds,
performs $N({\cal P}_i) = N({\cal P}_i)\setminus{\cal P}_k$, and stops
sending to ${\cal P}_k$ more chunks.
\end{comment}
\begin{comment}
computes a
``chunk-debt'', denoted by $d({\cal P}_k)$, that is incremented each
time a chunk is received from ${\cal P}_k$ and decremented each time a
chunk is sent to ${\cal P}_k$. If ${\cal P}_i$ verifies that $d({\cal
  P}_k)>D$ (the maximum debt), then ${\cal P}_i$ considers that ${\cal
  P}_k$ is unable to communicate with it, performs $N({\cal P}_i) =
N({\cal P}_i)\setminus{\cal P}_k$, and stops sending to ${\cal P}_k$
more chunks.
\end{comment}
