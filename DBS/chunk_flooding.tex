%%% Local Variables:
%%% mode: latex
%%% TeX-master: "<none>"
%%% End:

\label{sec:chunk_flooding}

\begin{comment}
\begin{figure*}
  %\includegraphics[width=0.75\textwidth]{chunk_generation_and_flooding}
  \fig{300}{3cm}{peer_chunk_flooding}
  \caption{Chunk flooding at peers.\label{fig:peer_chunk_flooding}}
\end{figure*}
\end{comment}

When a peer $P_k$ receives a chunk, $P_k$ floods a number of chunks to
one of its its neighbors (obviously, except the neighbor sender of the
chunk), using a prioritized round-robin schema, where those peers with
a low chunk debt are selected first. Every time $P_k$ sends a chunk to
$P_l$, $P_k$ runs $\text{debt}[P_l] = \text{debt}[P_l]+1$, and $P_l$
runs $\text{debt}[P_k] = \text{debt}[P_k]-1$.  Debs are clipped to
$\pm D$. In ideal circunstances, debs should be $0$. Obviously, a high
supportivity means a low debt, and viceversa.

\begin{notex}
  The prioritized round-robin neighbor selection has not yet been
  implmented.
\end{notex}

DBS is a content-unaware push-based protocol, where when a peer
received a chunk, it can be retransmitted to a large number of
neighbors. To avoid network congestion while flooding, sending peers
must perform some kind of data flow-control.

%Moreover, to achieve an ideal I/O ratio of $1$, peers should send one
%chunk for every received one.

Congestion control is performed by means of the basic idea of, {\sl if
  I have received a chunk, I should send a chunk}. It is easy to see
that, in a fully connected overlay, this allows to control the data
flow. However, peers can be ``connected'' with a variable number of
neighbors and therefore, if the splitter follows a pure round-robin
strategy, some peers can send more chunks that they receive.

The previous idea can be improved to handle a variable connectivity
degree. Each peer use an array of FIFO queues, $\text{pending}[]$,
indexed by the neighbor end-points, where each queue stores buffer
positions. Thus, if for example $\text{pending}[P_x]=\{11,22\}$,
chunks found at positions $11$ and $22$ of the buffer have to be sent
to peer $P_x$ when the prioritized round-robin scheduler used by the
peer select $P_x$. The scheduler selects a different neighbor for each
new received chunk.

\begin{figure*}
  \fig{250}{2.5cm}{team_0} \caption{A team has been created with a
    single monitor $M_0$ ($[\mathtt{hello}]$ messages are not
    shown). Chunks with numbers 0 and 1 (the time $t$ is measured in
    chunks-time) have been transmitted from the splitter $S$ to
    $M_0$. $F$ and $P$ represents the $\text{forward}[]$ and the
    $\text{pending}[]$ structures, respectively. The chunks stored in
    the buffer is shown under the entity.\label{fig:team_0}}
\end{figure*}

\begin{figure*}
  \fig{250}{2.5cm}{team_1} \caption{Peer $P_1$ joins the team (the
    $[\mathtt{hello}]$'s are not shown). In $M_0$, $F=\{M_0:[P_1]\}$
    because when $M_0$ receives the $[\mathtt{hello}]$ from $P_1$,
    $M_0$ is the origin peer for all chunks received from $S$ and
    $P_1$ is its neighbor. After that, when the chunk number 2 arrives
    to $M_0$ from $S$, an entry $P_1:2$ is created in $P\{\}$ for that
    chunk, and this entry is deleted when the chunk 2 is sent to
    $P_1$. When $P_1$ receives the chunk 2, $P_1$ inserts the entry
    $P_1:M_0$ in its forwarding table because $P_1$ considers to $M_0$
    as a neighbor. Notice that $P_1$ does not receive chunks 0 and 1
    because these chunks belong to a old (already played) section of
    the stream.\label{fig:team_1}}
\end{figure*}

An example of the flooding with congestion control algorithm has been
show in the Figs.~\ref{fig:team_0}, ..., \ref{fig:team_13}. Notice
that in this example, the debts have not been considered, and that all
messages are received successfully.

\begin{comment}
In each round, peers check if a chunk have been received from the rest
of peers of the team (${\cal P}_k\in {\cal T}_j)$). If not, peers send
a $[\mathtt{propagate}~{\cal P}_i]$ to one or more (possibly
to the rest of) peers of the team, where ${\cal P}_i$ is the origin peer
of the missing chunk. At this point, the process continues as
described in Section~\ref{dbs:chunk_flooding}.
\end{comment}

\begin{comment}
For each ${\cal P}_k\in N({\cal P}_i)$, ${\cal P}_i$ checks if a chunk
has been received from ${\cal P}_k$. If ${\cal P}_i$ detects that
${\cal P}_k$ has not sent a chunk to it during $L$ consecutive rounds,
performs $N({\cal P}_i) = N({\cal P}_i)\setminus{\cal P}_k$, and stops
sending to ${\cal P}_k$ more chunks.
\end{comment}
\begin{comment}
computes a
``chunk-debt'', denoted by $d({\cal P}_k)$, that is incremented each
time a chunk is received from ${\cal P}_k$ and decremented each time a
chunk is sent to ${\cal P}_k$. If ${\cal P}_i$ verifies that $d({\cal
  P}_k)>D$ (the maximum debt), then ${\cal P}_i$ considers that ${\cal
  P}_k$ is unable to communicate with it, performs $N({\cal P}_i) =
N({\cal P}_i)\setminus{\cal P}_k$, and stops sending to ${\cal P}_k$
more chunks.
\end{comment}

%When peers receive chunks from their splitter, they must flood them to
%their neighbors until the chunks are broadcasted to the whole team
%(Fig.~\ref{fig:chunk_generation_and_flooding}). Lets suppose that
%${\cal P}_k$ receives a chunk. In the case the sender is its splitter,
%${\cal P}_k$ floods the chunk to $N({\cal P}_k)$. However, if the
%sender is a peer ${\cal P}_m\in N({\cal P}_k)$, ${\cal P}_k$ adds
%${\cal P}_m$ to $N({\cal P}_k)$ if ${\cal P}_m$ is a new neighbor, and
%forwards the chunk to the rest of its neighborhood ${\cal P}_n\in
%N({\cal P}_k)\setminus{\cal P}_m$ if ${\cal P}_k$ is in the shortest
%between ${\cal P}_n$ and the origin peer ${\cal P}_i$ of the relayed
%chunk. This will be true if ${\cal P}_k$ is the gateway of ${\cal
%  P}_n$ to go from ${\cal P}_n$ to ${\cal P}_i$. Therefore, a flooding
%with prunning based on shortest path routing is used.



............

in DBS is very simple: if a new chunk is
received, peers forward (using the flooding with prunning algorithm
described in Sec~\ref{dbs:chunk_generation_and_flooding}) each
received chunk to the next peer of their list of peers (following a
round-robin pattern).

%Peers do not understand the content, but it is
%known that in order to achieve a I/O ratio of 1, peers should send one
%chunk for every received one, on average. To acomplish this, a ${\cal
%  P}_i$ creates a FIFO queue of chunks for each $N({\cal P}_i)$, and,
%for each received chunk, ${\cal P}_i$ forwards a queued chunk from
%each of these queues.

\begin{comment}
A ${\cal P}_i$ forwards one or more chunks if and only if it has
received a chunk. For each received chunk $c_j$, ${\cal P}_i$: 1)
creates a list $l_{c_j}$ with the contents of $N'({\cal P}_i)$, and 2)
sends $c_j$ to $l_{c_j}[0]$ (the first element), and removes
$l_{c_j}[0]$. For each chunk reception, Step 2) is repeated for all
the previously created lists while they are not exhausted.

A solution is a forwarding algorithm based on the following
idea. Peers manage a list of chunks, where every item is a 2-tuple
($c_k$, $P_l$). The field $c_k$ represents the chunk that must be
flooded (if the node that has delivered the chunk is the splitter,
$c_k$ must be relayed towards all the neighbors, otherwise, $c_k$ must
be sent to all the neighbors except the peer that delivered $c_k$),
and the field $P_l$ the last neighbor to which $c_k$ was sent. For
every chunk received, a new tuple is appended to the list of chunks
and the rest of tuples are updated. The field $c_k$ remains constant
but $P_l$ is replaced by the next peer in the list of neighbors for
every received chunk.
\end{comment}
