%%% Local Variables:
%%% mode: latex
%%% TeX-master: "<none>"
%%% End:

\label{sec:chunk_flooding}

\begin{comment}
\begin{figure*}
  %\includegraphics[width=0.75\textwidth]{chunk_generation_and_flooding}
  \imgw{300}{graphics/peer_chunk_flooding.svg}
  \caption{Chunk flooding at peers.\label{fig:peer_chunk_flooding}}
\end{figure*}
\end{comment}

DBS implements a push-based protocol. When a peer receives a chunk, it
can be retransmitted to a large number of neighbors (depending on the
number of different destinations in its forwarding table). Therefore,
even by controlling the chunk rate in the servers\footnote{Using DBS
  the data-flow can not be controlled in the peers or the splitter
  because they don't understand the stream.}, some kind of flow
control must be performed in order to reduce network congestion while
peers perform the flooding.

%Moreover, to achieve an ideal I/O ratio of $1$, peers should send one
%chunk for every received one.

\begin{figure*}
  \centering
  \subfloat[A full-connected overlay.]{\label{fig:full_mesh}
    \vbox{\myfig{graphics/full-mesh}{6cm}{250}}}
  \quad
  \subfloat[A star-shaped overlay.]{\label{fig:star}
    \vbox{\myfig{graphics/star}{6cm}{250}}}
  \caption{In a full-connected DBS team (see Subfig. (a)), all peers
    receive and send the same number of chunks. In a star-shaped DBS
    team such as the shown in the Subfig. (b), $P_1$ should send all
    the chunks of the stream to the rest of the team, except those
    that the splitter has send directly to the rest of peers.}
\end{figure*}

The congestion (caused by the use that the DBS nodes do of the
physical links) may be avoided by means of a basic idea: \textit{if I
  have received a chunk, I should send a chunk (not necessary to the
  sender)}. It is easy to see that, in a fully connected overlay (see
the Fig.~\ref{fig:full_mesh}), this allows to control the data
flow. However, in more realistic scenarios (such as those in which
firewalls and symmetric NATS are used), where the physical media
imposes communication restrictions, peers can not be ``connected''
with the rest the team, and therefore, if the splitter follows a pure
round-robin strategy, some peers can send more chunks that they
receive (see the Fig.~\ref{fig:star}). In this case, the simple rule
of sending a chunk for each received one does not work.

The previous idea can be adapted to handle a variable connectivity
degree if each peer uses a table of lists, $\mathtt{pending}[]$,
indexed by the neighbor's end-points, where each list stores the
positions in the buffer of those chunks that must be transmited to the
corresponding neighbor, the next time such neighbor be selected in the
flooding process. Thus, for example, if
$\mathtt{pending}[P_x]=\{11,22\}$, chunks found at positions $11$ and
$22$ of the buffer have to be sent to peer $P_x$ (when the pending
scheduler used by the peer selects $P_x$).

Each time a new chunk has arrived from the splitter (i.e., in each
round), the pending scheduler selects a different neighbor, using a
priority scheme, for each new received chunk. In this scheme, each
(destination) peer maintains a list of (neighbor) peers sorted by the
time that elapses (latency) since a chunk has been sent by the
splitter (to an origin) peer until this chunk has arrived to the
(destination) peer. With this information, peers selects first to
those neighbors with a smaller latency.

Notice that using this procedure, more than one chunk can be sent to a
neighbor in transmission burst, which could produce
congestion. However, except in very unbalanced overlays such as the
shown in the Fig.~\ref{fig:star}, the bursts use to be very short
(only one chunk in most of cases). As an advantage, if a burst is
produced, all the chunks of the burst travel between the two same
hosts, which usually increases the performance of the physical
routing.

An example of the temporal evolution of a team using this behaviour
has been described in the Figures \ref{fig:team_0}, \ref{fig:team_1}
and \ref{fig:team_2}. Notice that in this example, for the sake of
simplicity, a simple round-robing pending scheduler has been used.

\begin{figure}
  \myfig{graphics/team_0}{4cm}{250} \caption{A team has been created with a
    single monitor $M_0$ ($[\mathtt{hello}]$ messages are not
    shown). Chunks with indexes 0 and 1 (the time $t$ is measured in
    chunks-time) have been transmitted from the splitter $S$ to
    $M_0$. $f$ and $p$ represents the $\text{forward}[]$ and the
    $\text{pending}[]$ structures, respectively. The chunks stored in
    the buffer are shown below the entity.\label{fig:team_0}}
\end{figure}

\begin{figure}
  \myfig{graphics/team_1}{4cm}{250} \caption{At $t_2$, peer $P_1$ joins
    the team (the $[\mathtt{hello}]$'s are not shown). In $M_0$,
    $f=\{M_0:[P_1]\}$ because when $M_0$ receives the
    $[\mathtt{hello}]$ from $P_1$, $M_0$ is the origin peer for all
    chunks received from $S$ and $P_1$ is its neighbor. $P_1$ includes
    an entry $P_1:[M_0]$ in its forwarding table because $M_0$ is in
    the list of peers received from the splitter. After that, when the
    chunk number 2 arrives to $M_0$ from $S$, an entry $P_1:2$ is
    created in $P\{\}$ for that chunk, and this entry is deleted when
    the chunk 2 is sent to $P_1$.\label{fig:team_1}}
\end{figure}

\begin{figure}
  \myfig{graphics/team_2}{4cm}{300} \caption{$P_2$ joins the team. $M_0$
    and $P_1$ includes $P_2$ in their forwarding tables. The chunk 3
    is received by $P_1$ which decides to send it to $P_2$. Chunk 3
    remains as pending to be sent to $M_0$ when the next chunk is
    received by $P_1$. \label{fig:team_2}}
\end{figure}
%  $P_2$ is the origin peer for $M_0$ and $P_1$ because both of them are in the list of peers that $P_2$ receives from $S$.
    
\begin{figure}
  \myfig{graphics/team_3}{4.5cm}{330} \caption{Chunk 4 is received by $P_2$ which
    relays it to $P_1$, what relays chunk 3 to
    $M_0$.\label{fig:team_3}}
\end{figure}

\begin{figure}
  \myfig{graphics/team_4}{4.5cm}{330} \caption{Chunk 5 is received by $M_0$ which
    relays it to $P_2$.\label{fig:team_4}}
\end{figure}

\begin{notex}
  \begin{figure*}
    \myfig{graphics/team_5}{4cm}{350}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_6}{4cm}{380}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_7}{4cm}{320}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_8}{4cm}{200}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_9}{4cm}{280}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_10}{4cm}{380}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_11}{4cm}{210}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_12}{4cm}{210}
  \end{figure*}
  
  \begin{figure*}
    \myfig{graphics/team_13}{4cm}{210}
  \end{figure*}

\end{notex}

%An example of the flooding with congestion control algorithm has been
%show in the Figs.~\ref{fig:team_0}, ...
%Notice that in this example
%all messages are received successfully.
%As can be seen in the example, when a peer $P_k$ receives a chunk,
%$P_k$ floods a number of chunks to one of its its neighbors (obviously,
%except the neighbor sender of the chunk), using round-robin
%schema. The size of this set, what we call $\text{pending}$, depends
%on how many neighbors a peer has.

% Alternative: increment by +1 and decrement by /2

% Alternative: peers keep sorted the neighbors by debt, and the list
% is run from the beginning each time a chunk arrives from the
% splitter.

% Alternative: the debt is only incremented if the relayed chunk has
% been received from the splitter.

% Alternative: if between two consecutive chunks received from the
% splitter (a round), a peer does not receive a chunk from a neighbor
% with origin such neighbor, the peer is removed from the forwaring
% list.

\begin{comment}
In each round, peers check if a chunk have been received from the rest
of peers of the team (${\cal P}_k\in {\cal T}_j)$). If not, peers send
a $[\mathtt{propagate}~{\cal P}_i]$ to one or more (possibly
to the rest of) peers of the team, where ${\cal P}_i$ is the origin peer
of the missing chunk. At this point, the process continues as
described in Section~\ref{dbs:chunk_flooding}.
\end{comment}

\begin{comment}
For each ${\cal P}_k\in N({\cal P}_i)$, ${\cal P}_i$ checks if a chunk
has been received from ${\cal P}_k$. If ${\cal P}_i$ detects that
${\cal P}_k$ has not sent a chunk to it during $L$ consecutive rounds,
performs $N({\cal P}_i) = N({\cal P}_i)\setminus{\cal P}_k$, and stops
sending to ${\cal P}_k$ more chunks.
\end{comment}
\begin{comment}
computes a
``chunk-debt'', denoted by $d({\cal P}_k)$, that is incremented each
time a chunk is received from ${\cal P}_k$ and decremented each time a
chunk is sent to ${\cal P}_k$. If ${\cal P}_i$ verifies that $d({\cal
  P}_k)>D$ (the maximum debt), then ${\cal P}_i$ considers that ${\cal
  P}_k$ is unable to communicate with it, performs $N({\cal P}_i) =
N({\cal P}_i)\setminus{\cal P}_k$, and stops sending to ${\cal P}_k$
more chunks.
\end{comment}

%When peers receive chunks from their splitter, they must flood them to
%their neighbors until the chunks are broadcasted to the whole team
%(Fig.~\ref{fig:chunk_generation_and_flooding}). Lets suppose that
%${\cal P}_k$ receives a chunk. In the case the sender is its splitter,
%${\cal P}_k$ floods the chunk to $N({\cal P}_k)$. However, if the
%sender is a peer ${\cal P}_m\in N({\cal P}_k)$, ${\cal P}_k$ adds
%${\cal P}_m$ to $N({\cal P}_k)$ if ${\cal P}_m$ is a new neighbor, and
%forwards the chunk to the rest of its neighborhood ${\cal P}_n\in
%N({\cal P}_k)\setminus{\cal P}_m$ if ${\cal P}_k$ is in the shortest
%between ${\cal P}_n$ and the origin peer ${\cal P}_i$ of the relayed
%chunk. This will be true if ${\cal P}_k$ is the gateway of ${\cal
%  P}_n$ to go from ${\cal P}_n$ to ${\cal P}_i$. Therefore, a flooding
%with prunning based on shortest path routing is used.

%Peers do not understand the content, but it is
%known that in order to achieve a I/O ratio of 1, peers should send one
%chunk for every received one, on average. To acomplish this, a ${\cal
%  P}_i$ creates a FIFO queue of chunks for each $N({\cal P}_i)$, and,
%for each received chunk, ${\cal P}_i$ forwards a queued chunk from
%each of these queues.

\begin{comment}
A ${\cal P}_i$ forwards one or more chunks if and only if it has
received a chunk. For each received chunk $c_j$, ${\cal P}_i$: 1)
creates a list $l_{c_j}$ with the contents of $N'({\cal P}_i)$, and 2)
sends $c_j$ to $l_{c_j}[0]$ (the first element), and removes
$l_{c_j}[0]$. For each chunk reception, Step 2) is repeated for all
the previously created lists while they are not exhausted.

A solution is a forwarding algorithm based on the following
idea. Peers manage a list of chunks, where every item is a 2-tuple
($c_k$, $P_l$). The field $c_k$ represents the chunk that must be
flooded (if the node that has delivered the chunk is the splitter,
$c_k$ must be relayed towards all the neighbors, otherwise, $c_k$ must
be sent to all the neighbors except the peer that delivered $c_k$),
and the field $P_l$ the last neighbor to which $c_k$ was sent. For
every chunk received, a new tuple is appended to the list of chunks
and the rest of tuples are updated. The field $c_k$ remains constant
but $P_l$ is replaced by the next peer in the list of neighbors for
every received chunk.
\end{comment}
