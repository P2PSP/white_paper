% Emacs, this is -*-latex-*-

% Latency of the streaming system

\label{sec:latency}

\begin{figure}
  \centering
  \vbox{\myfig{graphics/ring}{6cm}{400}}
  \caption{A ring-shaped overlay.}
  \label{fig:ring}
\end{figure}

The latency of the streaming system is the delay that the streamed
signal needs to travel from a source to the peers. In a peer, the
latency, $t_l$, is the sum of the physical delay $t_p$ and the
buffering time $t_b$,
\begin{equation}
  \label{eq:t_l}
  t_l = t_p + t_b,
\end{equation}
where we can estimate
\begin{equation}
  \label{eq:t_b}
  t_b = Bt_c,
\end{equation}
which is the time that has elapsed since the peer receives two chunks
distanced at least $B$ positions in the buffer, where $B$ is the
buffer size (in chunks) and $t_c$ is the chunk time, considering that
$t_c$ is a constant\footnote{When $R$ is time-variying, $t_b$ would be
  the sum of the $t_c$ for each received chunk.}.

For real-time communications, $t_b$ should be as small as possible. To
achieve this we can reduce $t_c$ and/or $B$. Unfortunately, these
reductions generate other drawbacks. On the one hand, $t_c$ should be
large enough to reduce the overhead of the header of the transport
protocol. On the other hand, if $B$ is too small (for instance
$B<N^*$) the peer will not have enough space to buffer all the chunks
of a round. Thus, some chunks could overwrite others before they can
be played due to a low probability of receiving chunks in order. This
problem can also happen even if $N^*\leq B<2N^*$, because the maximum
jitter for a given peer (generated by DBS) that a chunk can experiment
is the sum of the maximum jitter produced by the splitter for this
peer, that can be $N^*$, and the maximum jitter produced by the team,
that can also be has high as $N^*$. Notice that this jitter is the
same for two extreme topologies of the overlay: (1) a full-connected
mesh (Fig.~\ref{fig:full_mesh}) or (2) a ring
(Fig.~\ref{fig:ring}). Therefore, peers have to use
\begin{equation}
  \label{eq:minimum_B}
  B\ge 2N^*.
\end{equation}

Lets analyze the latency in two interesting cases. Lets suppose that
the first received chunk is $x_1$ and that the rest of chunks of the
buffer of size $B$ are received, being the chunk $x_{1+B}$ the last
one (this is the ideal scenario).\footnote{Notice that all the chunks
  received with an number smaller than $x_1$ will be discarded.} In
this case, the buffering time can be estimated by
Ep.~\ref{eq:t_b}. Lets imagine now that after receiving $x_1$ the
chunk $x_{1+B}$ is received (the chunks $x_2, \cdots x_{1+B-1}$ have
been lost or delayed too much) and playing starts (probably, with
artifacts). In this case, the buffering time also corresponds with
Eq.~\ref{eq:t_b}, because the chunk $x_{1+B}$ was generated $B$ chunk
times after $x_1$ and we cannot wait more during the buffering time.

Notice that in this discussion, the time that passes from a incoming
peer request to join a team to a splitter until it accepts the peer,
and the time that the player's buffering time have not been
considered. However, both times should be, in general, significantly
lower than the buffering time.

After considering these two extreme situations, we can deduce that the
start-up-time does not depend on the chunk loss ratio during the
buffering-time (for loss ratios smaller than one), but only on $B$,
$t_c$ and $t_p$. Notice that, as a rule of thumb, the larger the
buffer size the lower the probability of losing chunks as a
consequence of a high $\Delta t_p$ (physical jitter) \leo{\{Incremento de la latencia??\}}.
