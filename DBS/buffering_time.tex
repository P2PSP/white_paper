% Emacs, this is -*-latex-*-

% Buffering time

\label{sec:buffering_time}
The buffering-time, estimated by
\begin{equation}
  \label{eq:t_b}
  t_b = Bt_c,  % # <--------------- Y el tiempo de ser servido por el splitter?
\end{equation}
determines how long the peers must wait for start playing the
chunks. For real-time communications, $t_b$ should be as small as
possible. To achieve this we can reduce $t_c$ and/or
$B$. Unfortunately, these reductions generate other drawbacks. On
the one hand, the overhead of the header of the transport protocol is
inversely proportional to $t_c$. Therefore, $t_c$ should be large
enough to \leochange{keep under control}{reduce} this overhead. On the other hand, if $B$
is too small (for instance $B<N^*$) the peer will not have enough
space to buffer all the chunks of a round. Thus, some chunks could
overwrite others before they can be played due to a low probability
of receiving chunks in order. This problem can also
happen even if $N^*\leq B<2N^*$, because the maximum jitter for a
given peer (generated by DBS) that a chunk can experiment is the sum
of the maximum jitter produced by the splitter for this peer, that can
be $N^*$, and the maximum jitter produced by the team, that also can
be $N^*$. Notice that this jitter is the same for the two extreme
topologies of the overlay: (1) a full-connected mesh
(Fig.~\ref{fig:full_mesh}) or (2) a ring (Fig.~\ref{fig:star}). Both topologies are possible in real scenarios \leorem{No hemos hablado de star antes}. Therefore, \leochange{users
should select}{peers have to use}
\begin{equation}
  \label{eq:minimum_B}
  B\ge 2N^*.
\end{equation}

Given a $N$ value, DBS peers may buffer a different number of chunks
that depends on the order in which chunks are received. If $x_1$ is
the (number of the) first received chunk (the first chunk to be
played), the buffering time finishes when a chunk with number equal or
greater than $x_{1+B}$ is received.\footnote{Notice that all the
  chunks received with an number smaller than $x_1$ will be discarded,
  and it can happens that some chunks
  are not received on time during the buffering-time. Therefore, it does not make sense to wait
  for $B$ chunks before stopping the buffering process.} Lets analyze
some interesting cases.

Lets suppose that the first received chunk is $x_1$ and that the rest
of chunks of the buffer of size $B$ are received, being the chunk
$x_{1+B}$ the last one (this is the ideal scenario). In this case, the
stream can be played without artifacts. Because the playing of the
chunks starts after the buffering process, the \gls{start-up-time}
experimented by users in the ideal case can be estimated by
\begin{equation}
  t_s = t_b + t_p,
  \label{eq:start-up-time}
\end{equation}
being $t_p$ the latency generated by the physical layer.

Imagine now one of the worst possible scenarios, in which after
receiving $x_1$ the chunk $x_{1+B}$ is received and the
chunks $x_2, \cdots x_{1+B-1}$ have been lost or delayed too much.
However (and considering that $t_c$ is a constant), the buffering-time
also corresponds with Eq.~\ref{eq:t_b}, because the chunk $x_{1+B}$
was generated $B$ chunk-times after $x_1$. Therefore, in this case the
start-up-time can be also estimated by Eq.~\ref{eq:start-up-time}.

After considering these two extreme situations, we can deduce that the
start-up-time does not depend on the chunk loss ratio during the
buffering-time (for loss ratios smaller than one), but only
on $B$, $t_c$ and $t_p$. Notice that, as a rule of thumb,  
the larger the buffer size the lower the probability of losing
chunks as a consequence of a high $\Delta t_p$\leorem{no definido} (physical jitter).
