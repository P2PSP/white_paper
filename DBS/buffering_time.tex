% Emacs, this is -*-latex-*-

% Buffering time

\label{sec:buffering_time}

$t_B$ determines how long the peers must wait for start playing the
chunks. For real-time communications, $t_B$ should be as small as
possible, and to achieve this we can reduce $t_C$ and
$B$. Unfortunately, these reductions generate another drawbacks. On
the one hand, the overhead of the header of the transport protocol is
inversely proportional to $t_C$, and therefore, $t_C$ should be
selected enough large to keep under control this overhead. On the
other hand, if $B$ is too small (for example, if $B<N^*$) the peer
will not have enought space to buffer all the chunks of a round, and
due to the probability of receiving all the chunks in order is very
small, some chunks will overwrite others before they can be
played. This problem can also happen even if $N^*\leq B<2N^*$, because
the maximum jitter for a given peer (generated by DBS) that a chunk
can experiment is the sum of the maximum jitter produced by the
splitter for this peer, that can be $N^*$, and the maximum jitter
produced by the team, $N^*$, in the case of a full-connected mesh such
as the shown in the Fig.~\ref{fig:full_mesh}. Therefore, users should
select $B\ge 2N^*$, and in general, the larger the buffer size, the
higher the $t_B$, but also the lower the probability of lossing
chunks.

Given a $N$ value, DBS peers may buffer a different number of chunks
that depends on the order in which chunks are received. If $x_1$ is
the (number of the) first received chunk (the first chunk to be
played), the buffering time finishes when a chunk with number equal or
greater than $x_{1+B}$ is received.\footnote{Notice that all received
  chunks with an number smaller than $x_1$ will be discarded, and that
  during the buffering time, it can happens that some chunks are not
  received on time. Therefore, it does not make sense to wait for $B$
  chunks before stopping the buffering process.} Lets analyze some
interesting cases.

Lets suppose that the first received chunk is $x_1$ and that the rest
of chunks of the buffer of size $B$ are received, being the chunk
$x_{1+B}$ the last one (this is the ideal scenario). In this case, the
stream can be played without artifacts. Because the playing of the
chunks starts after the buffering process, the end-latency
experimented by users in the ideal case would be $T=Bt_C+L$, being $L$
the latency generated by the physical transmission media.

Imagine now one of the worst possible scenarios, in which after
receiving $x_1$ the chunk $x_{1+B}$ is received. In this case, the
chunks $x_2, \cdots x_{1+B-1}$ have been lost (or delayed too much) by
the physical transmission media or the transmission protocol, but
again (and considering $t_C$ constant), the buffering time is
$t_B=Bt_C$ because the chunk $x_{1+B}$ was generated $B$ chunk times
after $x_1$. Therefore, in this case the end-latency is also
$T=Bt_C+L$.

After considering these two extreme situations, we can conclude that
the end-latency does not depend on the loss chunk ratio during the
buffering time (always that this ratio is smaller than one), but only
on $B$, $t_C$ and $L$.
