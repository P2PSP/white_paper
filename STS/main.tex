%%% Local Variables:
%%% mode: latex
%%% TeX-master: "<none>"
%%% End:

\label{sec:STS}

\begin{figure}
  \fig{600}{6cm}{icecast-P2PSP} \caption{A possible data-flow in an
  hybrid Icecast/P2PSP network. $S$ represents a splitter and $P_i$ a
  peer.\label{fig:icecast-P2PSP}}
\end{figure}

% On channels ...
P2PSP supposes that there is a collection of channels that are
broadcasted in parallel.\footnote{For example, channels in P2PSP is
  similar to channels in DVB (Digital Video Broadcasting): streams of
  media that are transmitted in parallel with the rest channels.} The
channels are available at one or more\footnote{Most client/server
  streaming services provide the possibility of building a CDN
  (Content Delivery Network), a tree of servers (sources) sending the
  same contents.} streaming \emph{servers}, and each channel has a
different URL (Universal Resource Locator), usually expressed as a
Web address with the structure:
\begin{verbatim}
  http://name.icecast.server/name_of_the_channel
\end{verbatim}

% Hybrid P2PSP/CDN structure
P2PSP does not perform data-flow control over the stream. The
transmission bit-rate between P2PSP entities is controlled by an
external server, which provides the
stream. Fig.~\ref{fig:icecast-P2PSP} shows an example of an hybrid
Icecast/P2PSP streaming overlay where several Icecast servers relay a
set of channels produced by a set of source-clients, directly or
through other servers. As can be seen, a listener (which usually plays
de stream) can be replaced by a \emph{splitter}, a P2PSP entity that
sends the received stream (a single channel) to a set of P2PSP
\emph{peers}, called \emph{team}. Notice that, considering that a
player can be attached to each peer, the scalability of the hybrid
alternative is much higher than the pure client/server architecture.
%As the listeners, the peers receive
%a copy of a stream

%All peers of a team receive the same
%stream, as the rest of listeners. The only difference is that, in the
%case of a listener, the stream is provided by a server exclusively,
%and in a P2PSP team, the stream is gathered from the splitter and the
%rest of peers of the team.

% A load balancer for Icecast
Usually, users are mainly interested in receiving channels, not in
knowing the topology of the overlay. In a pure Icecast system, users
request the channels directly to the servers. Unfortunately, this
simple procedure has a drawback: normally, users does not know the
load of the servers nor their distance to the user's player. This
problem can be solved using a \emph{load balancer}. The listeners (and
the splitters) which know the URL of the required channel, connects
first to the load balancer which redirects them (with a HTTP 302 code)
to a suitable server.

% An extended load balancer
This idea can be extended to take advantage of the existence of P2PSP
teams in hybrid overlays, where some users run a local peer to
retrieve a copy of the stream. In this case, the player
communicates\footnote{This is a reliable communication. Reliable
  messages are transmitted over TCP. On the other hand, unreliable
  messages, which can be lost in transit, are transmitted over UDP. In
  any case, the reception of a packet is a permanent blocking action,
  at least a timeout is indicated.} with the load balancer to tell it
that there is a peer running in the same host that the player. Thus,
when the player requests to the load balancer the reception of the
stream, the load balancer redirects it towards the local
peer.\footnote{If there is a peer running in the same host that the
  player, the load balancer will redirect the player to the local
  peer, and the channel served to the player will be the one passed as
  a parameter when the peer was instantiated. In other words, the
  channel specified by the player in its request to the load balancer
  and the peer will be ignored.}
%The situation would be identical if the user would run a HTTP proxy
%locally.

% The splitters tracker
When a network entity (usually, a player) connects with a peer, this
must connects with a splitter which is transmitting the stream
selected by the user. Depending on maximum number of potential peers
that are retrieving the same channel, more than one splitter could be
available. This information is served by a \emph{splitters tracker},
and generated when a new splitter is instantiated. The tracker keeps
updated a table indexed by names of channels, where every entry
contains a list of splitters that are transmitting the corresponding
channel. Therefore, to find a suitable splitter, the peer first
retrieves from the splitters tracker a list of splitters. Then, the
peer tries to connect with all the splitters\footnote{Those splitters
  with full teams should not accept new connections from peers.} in
parallel, and the fastest connection determines the selected splitter
(the rest of successful connections are closed). This procedure should
select the ``closest'' splitter to the peer in terms of network
latency.

%\begin{figure*}
%  \fig{800}{8cm}{STS} \caption{Procedures run by the load balancer,
%    the splitters tracker, the splitters and the peers. By definition,
%    tasks (infinite loops) and threads (finite loops) are run in
%    parallel with other tasks, threads and functions. All instances
%    start running the $\text{main}()$ function. \label{fig:STS}}
%\end{figure*}

The splitters send to the tracker messages with the current number $t$
of ``spaces'' in their teams. When a splitters (and its team) is
created, it sends a message $[t]$ ($t \leq N^*-1$) where $N^*$ is the
maximum team size\footnote{A parameter decided by the overlay
administrator.} for that splitter (notice that at least one monitor
peer must be in each team, attached manually). When the team is full,
the splitter sends a message $[t=0]$. When the team is shutted
down, the splitter sends $[t=-1]$. Using the number of free spaces in
each team, the splitters tracker can perform some load balacing among
the teams, by including only those teams with more room in the
$[\text{list of splitters}]$ response.

\begin{figure*}
  \fig{900}{9cm}{STS_example}
  \caption{Timeline example of an STS interaction.\label{fig:STS_example}}
\end{figure*}

%The Fig.~\ref{fig:STS} describes in more accurately the algorithms run
%by the different entities that participate in STS.
The Fig.~\ref{fig:STS_example} shown an example of an STS
communication. When the peer is created, it sends a $[\mathtt{hello}]$
message to the load balancer telling that a player is running in the
same host that the peer and therefore, the peers should be redirected
towards it. When the splitters are instantiated, they send their teams
sizes ($1$, supposing that only one monitor has added to each
team). The player is redirected to the peer which, after receiving the
list of splitters from the tracker, selects the Splitter$_1$ because
answers first than the Splitter$_2$.

\begin{comment}
% The channels tracker
When a user runs a P2PSP's media player, it connects to a channels
tracker (see Fig.~\ref{fig:TTS}). The tracker returns the list of
channels currently broadcasted, and the user must choose one of
them. This selection is sent back to the channels tracker, which
responds with the end-point of a teams tracker. There is exactly one
teams tracker per channel. Every time a new channel is created, a new
teams tracker is spawned and assigned to this channel. All teams
tracked by the same teams tracker broadcast the same channel.

% The teams tracker
Then, the player connects to the teams tracker provided by the
channels tracker, that should returns a list end-points where the
corresponding splitters, with space in their teams, are listening. If
none of the splitters that are broadcasting the channel has room for
the incoming peer, the teams tracker will create a new empty team of
the channel, and will return the splitter's end-point.

% Creating channels (inserting entried in the channel2tracker table)
To create a new channel, a new source-client must
send\footnote{Typically, access control over who can send a channel is
  provided by the Icecast servers.} the stream to a server (or a relay
server), a new teams tracker must be created, and the teams tracker
should spawn a splitter and an empty\footnote{An empty team is built
  with a splitter and at least one monitor peer.} connected to a
source\footnote{Notice that we are supposing that at least a source
  can handle a new listener. If this is not true, the splitter will
  not be able to connect to a source, it will inform to the teams
  tracker, which will inform to the player, which should inform to the
  user with the warning ``The maximum number of concurrent users has
  been reached for this channel. Please, wait or select a different
  channel''.} team.

% Searching the closest team 
With this information (a list of splitters with room in their teams),
the player connects with all the splitters in parallel and the fastest
connection determines the final selected splitter. The rest of
connections are closed. This procedure should select the ``closest''
splitter to the player in terms of network latency.

% Spawning the peer
Finally, the player spawns a peer and the team-attaching process, as
described in Sec.~\ref{sec:DBS}, starts.

% As can be seen in Fig.~\ref{fig:TTS}, to find a team given a
% channel, several entities must run different tasks. The player,
% controlled by the user, must obtain a splitter's end-point. In the
% channels tracker, a task listens to the players and serves, first
% the list of current channels and second, a teams tracker dedicated
% to such channel. Two tasks in the teams tracker, one for serving the
% splitters and another for keeping updated the list of splitters with
% room in its teams, must be run.

\end{comment}
